{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parks_df = pd.read_parquet(\"../../data/nps/nps_public_data_parks.parquet\")\n",
    "parks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a snippet to fetch all the parks in Utah and order the results by the park name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df[parks_df[\"states\"].str.contains(\"UT\")].sort_values(by=\"fullName\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a query to fetch all the National Parks that cross state boundaries. \n",
    "\n",
    "Hint: `parks.states` is a string representation of a list, i.e. `UT,CA,NC`. The `parks` table includes parks that aren't National Parks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df[\n",
    "    (parks_df[\"designation\"].str.contains(\"National Park\"))\n",
    "    & (parks_df[\"states\"].str.contains(\",\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all national parks, return the `states` column as a `STRUCT` type with each element as a state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df[\"states_list\"] = list(parks_df[\"states\"].str.split(\",\"))\n",
    "parks_df[\"states_list\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which parks are in either Montana or Wyoming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_parks_df = parks_df[parks_df[\"designation\"].str.contains(\"National Park\")]\n",
    "\n",
    "national_parks_df[\n",
    "    national_parks_df[\"states\"].str.contains(\"MT\")\n",
    "    | national_parks_df[\"states\"].str.contains(\"WY\")\n",
    "].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about _both_ Montana _and_ wyoming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_parks_df[\n",
    "    national_parks_df[\"states\"].str.contains(\"MT\")\n",
    "    & national_parks_df[\"states\"].str.contains(\"WY\")\n",
    "].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which park is in the greatest number of states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_parks_df[\"num_states\"] = national_parks_df[\"states_list\"].str.len()\n",
    "\n",
    "national_parks_df[[\"fullName\", \"num_states\", \"states_list\"]].sort_values(\n",
    "    by=\"num_states\", ascending=False\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how many parks are in each \"group\" of state border-crossings?\n",
    "\n",
    "Hint: we're grouping by the _number_ of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_parks_df.groupby(\"num_states\")[\"fullName\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the percentage share of the total? Hint: window functions might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = national_parks_df.groupby(\"num_states\")[\"fullName\"].count().sum()\n",
    "\n",
    "national_parks_df.groupby(\"num_states\")[\"fullName\"].count() / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query that returns the _largest_ campsite in each park. Note: when creating a `total_campsites` column, you might have to fill `na` columns with zero. You might also find the `idxmax` function helpful for this exercise ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "campsites_df = pd.read_parquet(\"../../data/nps/nps_public_data_campgrounds.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_parks_df[\"parkCode\"] = national_parks_df[\"parkCode\"].astype(\"str\")\n",
    "campsites_df[\"parkCode\"] = campsites_df[\"parkCode\"].astype(\"str\")\n",
    "\n",
    "join_df = national_parks_df.merge(\n",
    "    campsites_df, how=\"inner\", on=[\"parkCode\"], suffixes=(\"_park\", \"_camp\")\n",
    ")\n",
    "\n",
    "join_df[\"total_campsites\"] = (\n",
    "    join_df[\"numberOfSitesFirstComeFirstServe\"] + join_df[\"numberOfSitesReservable\"]\n",
    ")\n",
    "\n",
    "join_df[\"total_campsites\"] = join_df[\"total_campsites\"].fillna(value=0)\n",
    "\n",
    "join_df[[\"name_park\", \"name_camp\", \"total_campsites\"]].loc[\n",
    "    join_df.groupby(\"fullName\")[\"total_campsites\"].idxmax()\n",
    "].sort_values(by=\"total_campsites\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you'll be in California this spring and have time for three National Parks visits. How many combinations of national parks can you visit? Can you return the combinations in a list ordered by the name of the first park?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "unique_ca_parks = national_parks_df[national_parks_df[\"states\"].str.contains(\"CA\")][\n",
    "    \"name\"\n",
    "].unique()\n",
    "\n",
    "combinations_ca_parks = list(itertools.combinations(unique_ca_parks, 3))\n",
    "\n",
    "sorted(combinations_ca_parks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the combinations in alphabetical order, that is, the first letter of each visit occurs in the order of the alphabet, e.g. `[C]hannel Islands, [D]eath Valley, [J]oshua Tree` would satisfy that condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[c for c in combinations_ca_parks if list(c) == sorted(c)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull in our alerts data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alerts_df = pd.read_parquet(\"../../data/nps/nps_public_data_alerts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column 'alert_date' that converts `lastIndexedDate` to a date (no time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "alerts_df[\"alert_date\"] = pd.to_datetime(alerts_df[\"lastIndexedDate\"]).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping by `alert_date` and `category` return a `grouby` that counts the number of alerts, per category per day. Be sure to reset the index so that each row has an `alert_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_df.groupby([\"alert_date\", \"category\"])[\"title\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the `parks_df` on `parkCode` and create a new df grouping alerts by date and park."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alerts_joined = parks_df.merge(\n",
    "    alerts_df, how=\"inner\", on=[\"parkCode\"], suffixes=(\"_park\", \"_alert\")\n",
    ")\n",
    "\n",
    "alerts_agg = (\n",
    "    alerts_joined.groupby([\"alert_date\", \"name\"])[\"title\"].count().reset_index()\n",
    ")\n",
    "\n",
    "alerts_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a dataframe that contains the latest alert date for each park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_recent_alerts = alerts_agg.groupby(\"name\")[\"alert_date\"].max()\n",
    "\n",
    "most_recent_alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our joined df, let's tie it all together. Let's analyze alerts by state. Note that this means one alert can apply to multiple states. Can you create a dataframe of alerts counts by day, grouped by state? Hint: remember how we exploded states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_alerts = (\n",
    "    alerts_joined.explode(\"states_list\")\n",
    "    .groupby([\"alert_date\", \"states_list\"])[\"title\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "state_alerts.rename({\"title\": \"num_alerts\"}, inplace=True, axis=1)\n",
    "\n",
    "state_alerts.tail()\n",
    "\n",
    "dates = pd.DataFrame(\n",
    "    pd.date_range(\n",
    "        start=\"2023-01-01\",\n",
    "        end=state_alerts[\"alert_date\"].max(),\n",
    "        freq=\"D\",\n",
    "    )\n",
    ").rename(columns={0: \"alert_date\"})\n",
    "\n",
    "state_alerts[\"alert_date\"] = pd.to_datetime(state_alerts[\"alert_date\"])\n",
    "\n",
    "alert_dates = dates.merge(state_alerts, how=\"left\", on=\"alert_date\").fillna(0).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_dates_reindexed = alert_dates.set_index(\"alert_date\")\n",
    "\n",
    "rolling_alerts_28 = (\n",
    "    alert_dates_reindexed.groupby([\"states_list\"])[\"num_alerts\"]\n",
    "    .rolling(window=28)\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"num_alerts\": \"rolling_28\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(\n",
    "    data_frame=rolling_alerts_28,\n",
    "    x=\"alert_date\",\n",
    "    y=\"rolling_28\",\n",
    "    color=\"states_list\",\n",
    "    title=f\"Cumulative Alerts\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_alerts_28[rolling_alerts_28.states_list == \"UT\"].sort_values(by=\"alert_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil-cdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
