{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first difference we'll note with Python and Pandas is that instead of selecting from a database, we'll be reading in individual files to query with Pandas. This is primarily a byproduct of _when_ we use tools like Pandas. Frequently, this _will not_ be against a database, as with SQL, but against structured or semi-structured data stored in JSON, CSV, parquet, or other formats. \n",
    "\n",
    "Here we're using `read_parquet` to pull in the corresponding parquet file for the Parks dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_df = pd.read_parquet(\"../../data/nps/nps_public_data_parks.parquet\")\n",
    "parks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some similar operations to our DuckDB example— renaming a column and expanding the `STRUCT` column, `operatingHours`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "rename_dict = {\"operatingHours\": \"operating_hours\"}\n",
    "\n",
    "# Note that rename requires casing and mnames to be precicely correct..\n",
    "# It often won't throw errows if they are not, but it also won't rename the columns, so be sure to check.\n",
    "\n",
    "parks_df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "pprint(list(parks_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can expect the operating_hours column to better understand the data structure\n",
    "# Let's look at a sample record\n",
    "\n",
    "parks_df[\"operating_hours\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This record is a _list_ of dictionaries... That means we can't just unpack the values since each row could have more than one value. DuckDB handled that for us, but Pandas won't! \n",
    "\n",
    "This will be a theme throughout the remainder of the course. Certain tools might be more effective than others. It's our job to figure out what makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The explode() method will unpack our list into individual rows\n",
    "parks_df_exploded = parks_df.explode(\"operating_hours\")\n",
    "parks_df_exploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can unnest the operating hours column with `pd.json_normalize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_operating_hours_df = pd.json_normalize(parks_df_exploded[\"operating_hours\"])\n",
    "\n",
    "park_operating_hours_df.rename(\n",
    "    columns={\"name\": \"category\", \"description\": \"operating_hours_description\"},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "park_operating_hours_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we have a separate dataframe! To join it back to our original df, we can use `pd.concat` with `axis=1`— that tells us to concatenate our dataframes by column.\n",
    "\n",
    "We also have to `reset_index` for each of our dataframes to ensure a true join. Yes, this is confusing. Unfortunately, the only way to learn is through trial, error, [StackOverflow](https://stackoverflow.com/a/47657006), and the Pandas [documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the dataframes' order are identical, we can simply join them\n",
    "parks_with_hours_df = pd.concat(\n",
    "    [\n",
    "        parks_df_exploded.reset_index(drop=True),\n",
    "        park_operating_hours_df.reset_index(drop=True),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "parks_with_hours_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then perform filters, as in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parks_with_hours_df[parks_with_hours_df[\"category\"] == \"Hours of Operation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the filter syntax:\n",
    "\n",
    "```\n",
    "df[\n",
    "    df[column] [LOGICAL MODIFIER] [VALUE]\n",
    "]\n",
    "```\n",
    "\n",
    "we're taking the dataframe and applying a _mask_ where the column satisfies a certain condition. This is fundamentally different from filtering in SQL and can take some getting used to.\n",
    "\n",
    "For multiple filters, we can use the logical operators `& |` and parentheses for grouping, for example:\n",
    "\n",
    "```\n",
    "df[\n",
    "    (CONDITION 1 & CONDITION 2) | CONDITION 3\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the hours of operation for Theodore Roosevelt National Park based on the description\n",
    "\n",
    "parks_with_hours_df[\n",
    "    (parks_with_hours_df[\"category\"] == \"Hours of Operation\")\n",
    "    & (\n",
    "        parks_with_hours_df[\"operating_hours_description\"].str.contains(\n",
    "            \"Theodore Roosevelt\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# Note the multiline formatting— this can directly improve readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinct values can be accessed with the `unique()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(parks_with_hours_df[\"standardHours.thursday\"].unique())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas equivalent of `CASE WHEN` is accessed through the numpy library `.where` function. If follows a similar pattern. \n",
    "\n",
    "We'll set a condition to select for— `parks_with_hours_df['standardHours.monday'] == 'unknown'`, if true, we'll return `Closed`. If not, we'll return the existing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CASE monday WHEN 'unknown' THEN 'Closed' ELSE monday END as monday_hours,\n",
    "\n",
    "parks_with_hours_df[\"monday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.monday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.monday\"],\n",
    ")\n",
    "parks_with_hours_df[\"tuesday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.tuesday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.tuesday\"],\n",
    ")\n",
    "parks_with_hours_df[\"wednesday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.wednesday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.wednesday\"],\n",
    ")\n",
    "parks_with_hours_df[\"thursday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.thursday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.thursday\"],\n",
    ")\n",
    "parks_with_hours_df[\"friday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.friday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.friday\"],\n",
    ")\n",
    "parks_with_hours_df[\"saturday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.saturday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.saturday\"],\n",
    ")\n",
    "parks_with_hours_df[\"sunday_hours\"] = np.where(\n",
    "    parks_with_hours_df[\"standardHours.sunday\"] == \"unknown\",\n",
    "    \"Closed\",\n",
    "    parks_with_hours_df[\"standardHours.sunday\"],\n",
    ")\n",
    "\n",
    "\n",
    "parks_with_hours_df[\"open_seven_days_a_week\"] = np.where(\n",
    "    (\n",
    "        (parks_with_hours_df[\"monday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"tuesday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"wednesday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"thursday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"friday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"saturday_hours\"] != \"Closed\")\n",
    "        & (parks_with_hours_df[\"sunday_hours\"] != \"Closed\")\n",
    "    ),\n",
    "    True,\n",
    "    False,\n",
    ")\n",
    "\n",
    "cols_to_select = [\n",
    "    \"fullName\",\n",
    "    \"open_seven_days_a_week\",\n",
    "    \"monday_hours\",\n",
    "    \"tuesday_hours\",\n",
    "    \"wednesday_hours\",\n",
    "    \"thursday_hours\",\n",
    "    \"friday_hours\",\n",
    "    \"saturday_hours\",\n",
    "    \"sunday_hours\",\n",
    "]\n",
    "\n",
    "parks_with_hours_df[cols_to_select].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_seven_days_df = parks_with_hours_df[parks_with_hours_df[\"open_seven_days_a_week\"]]\n",
    "\n",
    "open_seven_days_df[cols_to_select].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lil-cdt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
